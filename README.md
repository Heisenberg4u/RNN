# RNN
The problem with CNN is it doesn't respect sequence of words so we use RNN which is helpful in sequence processing, data is non reshuffleable here and it do respect sequence of words but in RNN we have to deal a problem named as 'Vanishing Gradient ' problem and to tackle this we use a better and advance version of RNN  named LSTM(Long Short Term Memory) and I have uploaded the codefile only as the keras will itself import the dataset(Imdb) for which I have wrote the codes. Please use google colab to run the codes with the edits you want.  
