{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM .ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmKUiNF0JxVK"
      },
      "source": [
        "# sklearn is the machine learning model library, keras(tensorflow) is deep learning model library\n",
        "# pandas(numpy)\n",
        "import numpy as np\n",
        "from keras.datasets import imdb # Balanced dataset, 1 and 0 classes are balanced\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense # Simple neural network layer\n",
        "from keras.layers import LSTM # sequence processing\n",
        "from keras.layers.embeddings import Embedding # to convert words into vectors for processing\n",
        "from keras.preprocessing import sequence # Zero padding of small sentences to be able to create batches of sentence data of different lengths."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YtjJRasJxVQ",
        "outputId": "51dba3b0-d5a2-4dd2-ea42-2c07041fec97"
      },
      "source": [
        "# load the dataset but only keep the top n words, zero the rest\n",
        "# tf-idf .. unique word size -- 1600\n",
        "top_words = 5000 # vocabulary size, number of unique words will be 5000 >> 1600\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words) # select those sentences such that number of unique words <= 5000.\n",
        "print(X_train.shape)\n",
        "print(X_train[0])\n",
        "print(len(X_train[0]))\n",
        "print(X_test.shape)\n",
        "\n",
        "# All that we need is the serial number of a word in the unique word list and the actual word is not required for saving data space and also processing"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(25000,)\n",
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
            "218\n",
            "(25000,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQZlNMOWY1XN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9088789-5edb-456a-c79c-452727c009e9"
      },
      "source": [
        "unique_word_dictionary = imdb.get_word_index()\n",
        "# unique_word_dictionary"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uk85Q1xdZTjF"
      },
      "source": [
        "reverse_index_word_dictionary = {}\n",
        "for i in unique_word_dictionary:\n",
        "  if not unique_word_dictionary[i] in reverse_index_word_dictionary:\n",
        "    reverse_index_word_dictionary[unique_word_dictionary[i]] = i"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNfgVf6SZvCv"
      },
      "source": [
        "# reverse_index_word_dictionary"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRIeBGKnZFY2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a76aa6fc-e503-4574-818b-7961a0b0b4a6"
      },
      "source": [
        "for i in X_train[1]:\n",
        "  print(reverse_index_word_dictionary[i])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the\n",
            "thought\n",
            "solid\n",
            "thought\n",
            "and\n",
            "do\n",
            "making\n",
            "to\n",
            "is\n",
            "spot\n",
            "nomination\n",
            "and\n",
            "while\n",
            "he\n",
            "of\n",
            "jack\n",
            "in\n",
            "where\n",
            "picked\n",
            "as\n",
            "getting\n",
            "on\n",
            "was\n",
            "did\n",
            "hands\n",
            "fact\n",
            "characters\n",
            "to\n",
            "always\n",
            "life\n",
            "thrillers\n",
            "not\n",
            "as\n",
            "me\n",
            "can't\n",
            "in\n",
            "at\n",
            "are\n",
            "br\n",
            "of\n",
            "sure\n",
            "your\n",
            "way\n",
            "of\n",
            "little\n",
            "it\n",
            "strongly\n",
            "random\n",
            "to\n",
            "view\n",
            "of\n",
            "love\n",
            "it\n",
            "so\n",
            "and\n",
            "of\n",
            "guy\n",
            "it\n",
            "used\n",
            "producer\n",
            "of\n",
            "where\n",
            "it\n",
            "of\n",
            "here\n",
            "icon\n",
            "film\n",
            "of\n",
            "outside\n",
            "to\n",
            "don't\n",
            "all\n",
            "unique\n",
            "some\n",
            "like\n",
            "of\n",
            "direction\n",
            "it\n",
            "if\n",
            "out\n",
            "her\n",
            "imagination\n",
            "below\n",
            "keep\n",
            "of\n",
            "queen\n",
            "he\n",
            "and\n",
            "to\n",
            "makes\n",
            "this\n",
            "stretch\n",
            "and\n",
            "of\n",
            "solid\n",
            "it\n",
            "thought\n",
            "begins\n",
            "br\n",
            "and\n",
            "and\n",
            "budget\n",
            "worthwhile\n",
            "though\n",
            "ok\n",
            "and\n",
            "and\n",
            "for\n",
            "ever\n",
            "better\n",
            "were\n",
            "and\n",
            "and\n",
            "for\n",
            "budget\n",
            "look\n",
            "kicked\n",
            "any\n",
            "to\n",
            "of\n",
            "making\n",
            "it\n",
            "out\n",
            "and\n",
            "follows\n",
            "for\n",
            "effects\n",
            "show\n",
            "to\n",
            "show\n",
            "cast\n",
            "this\n",
            "family\n",
            "us\n",
            "scenes\n",
            "more\n",
            "it\n",
            "severe\n",
            "making\n",
            "and\n",
            "to\n",
            "and\n",
            "finds\n",
            "tv\n",
            "tend\n",
            "to\n",
            "of\n",
            "and\n",
            "these\n",
            "thing\n",
            "wants\n",
            "but\n",
            "and\n",
            "an\n",
            "and\n",
            "cult\n",
            "as\n",
            "it\n",
            "is\n",
            "video\n",
            "do\n",
            "you\n",
            "david\n",
            "see\n",
            "scenery\n",
            "it\n",
            "in\n",
            "few\n",
            "those\n",
            "are\n",
            "of\n",
            "ship\n",
            "for\n",
            "with\n",
            "of\n",
            "wild\n",
            "to\n",
            "one\n",
            "is\n",
            "very\n",
            "work\n",
            "dark\n",
            "they\n",
            "don't\n",
            "do\n",
            "dvd\n",
            "with\n",
            "those\n",
            "them\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVldG4B9fMqI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc1730b6-a164-4e5a-fd3c-ff68d1705d7e"
      },
      "source": [
        "len(X_train[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "218"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjyGaJlmfZjA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e20505f8-8df2-432c-c75d-0fa6244afdc1"
      },
      "source": [
        "X_train[0]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 14,\n",
              " 22,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 973,\n",
              " 1622,\n",
              " 1385,\n",
              " 65,\n",
              " 458,\n",
              " 4468,\n",
              " 66,\n",
              " 3941,\n",
              " 4,\n",
              " 173,\n",
              " 36,\n",
              " 256,\n",
              " 5,\n",
              " 25,\n",
              " 100,\n",
              " 43,\n",
              " 838,\n",
              " 112,\n",
              " 50,\n",
              " 670,\n",
              " 2,\n",
              " 9,\n",
              " 35,\n",
              " 480,\n",
              " 284,\n",
              " 5,\n",
              " 150,\n",
              " 4,\n",
              " 172,\n",
              " 112,\n",
              " 167,\n",
              " 2,\n",
              " 336,\n",
              " 385,\n",
              " 39,\n",
              " 4,\n",
              " 172,\n",
              " 4536,\n",
              " 1111,\n",
              " 17,\n",
              " 546,\n",
              " 38,\n",
              " 13,\n",
              " 447,\n",
              " 4,\n",
              " 192,\n",
              " 50,\n",
              " 16,\n",
              " 6,\n",
              " 147,\n",
              " 2025,\n",
              " 19,\n",
              " 14,\n",
              " 22,\n",
              " 4,\n",
              " 1920,\n",
              " 4613,\n",
              " 469,\n",
              " 4,\n",
              " 22,\n",
              " 71,\n",
              " 87,\n",
              " 12,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 38,\n",
              " 76,\n",
              " 15,\n",
              " 13,\n",
              " 1247,\n",
              " 4,\n",
              " 22,\n",
              " 17,\n",
              " 515,\n",
              " 17,\n",
              " 12,\n",
              " 16,\n",
              " 626,\n",
              " 18,\n",
              " 2,\n",
              " 5,\n",
              " 62,\n",
              " 386,\n",
              " 12,\n",
              " 8,\n",
              " 316,\n",
              " 8,\n",
              " 106,\n",
              " 5,\n",
              " 4,\n",
              " 2223,\n",
              " 2,\n",
              " 16,\n",
              " 480,\n",
              " 66,\n",
              " 3785,\n",
              " 33,\n",
              " 4,\n",
              " 130,\n",
              " 12,\n",
              " 16,\n",
              " 38,\n",
              " 619,\n",
              " 5,\n",
              " 25,\n",
              " 124,\n",
              " 51,\n",
              " 36,\n",
              " 135,\n",
              " 48,\n",
              " 25,\n",
              " 1415,\n",
              " 33,\n",
              " 6,\n",
              " 22,\n",
              " 12,\n",
              " 215,\n",
              " 28,\n",
              " 77,\n",
              " 52,\n",
              " 5,\n",
              " 14,\n",
              " 407,\n",
              " 16,\n",
              " 82,\n",
              " 2,\n",
              " 8,\n",
              " 4,\n",
              " 107,\n",
              " 117,\n",
              " 2,\n",
              " 15,\n",
              " 256,\n",
              " 4,\n",
              " 2,\n",
              " 7,\n",
              " 3766,\n",
              " 5,\n",
              " 723,\n",
              " 36,\n",
              " 71,\n",
              " 43,\n",
              " 530,\n",
              " 476,\n",
              " 26,\n",
              " 400,\n",
              " 317,\n",
              " 46,\n",
              " 7,\n",
              " 4,\n",
              " 2,\n",
              " 1029,\n",
              " 13,\n",
              " 104,\n",
              " 88,\n",
              " 4,\n",
              " 381,\n",
              " 15,\n",
              " 297,\n",
              " 98,\n",
              " 32,\n",
              " 2071,\n",
              " 56,\n",
              " 26,\n",
              " 141,\n",
              " 6,\n",
              " 194,\n",
              " 2,\n",
              " 18,\n",
              " 4,\n",
              " 226,\n",
              " 22,\n",
              " 21,\n",
              " 134,\n",
              " 476,\n",
              " 26,\n",
              " 480,\n",
              " 5,\n",
              " 144,\n",
              " 30,\n",
              " 2,\n",
              " 18,\n",
              " 51,\n",
              " 36,\n",
              " 28,\n",
              " 224,\n",
              " 92,\n",
              " 25,\n",
              " 104,\n",
              " 4,\n",
              " 226,\n",
              " 65,\n",
              " 16,\n",
              " 38,\n",
              " 1334,\n",
              " 88,\n",
              " 12,\n",
              " 16,\n",
              " 283,\n",
              " 5,\n",
              " 16,\n",
              " 4472,\n",
              " 113,\n",
              " 103,\n",
              " 32,\n",
              " 15,\n",
              " 16,\n",
              " 2,\n",
              " 19,\n",
              " 178,\n",
              " 32]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KokipfsnhZ6D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "313fdd6c-8eb3-4cb4-c9b0-2472958049ba"
      },
      "source": [
        "np.sum(y_test)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12500"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW2vr4zjJxVl",
        "outputId": "e176e365-eca6-4354-c2be-a19e74f12337"
      },
      "source": [
        "# truncate and pad input sequences\n",
        "max_review_length = 500 # maximum 500 words in a review/ per example (very big reviews will take lot of computation time)\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length) # do sequence zero padding on train data so that all sentences are of 500 words\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length) # do sequence zero padding on test data so that all sentences are of 500 words\n",
        "# Reviews of length > 500, will get truncated while\n",
        "# Reviews of length < 500, will get zero padded\n",
        "# so the end result is that all sentences will of lenghth 500 words.\n",
        "print(X_train[0])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    1   14   22   16   43  530  973 1622 1385   65  458 4468\n",
            "   66 3941    4  173   36  256    5   25  100   43  838  112   50  670\n",
            "    2    9   35  480  284    5  150    4  172  112  167    2  336  385\n",
            "   39    4  172 4536 1111   17  546   38   13  447    4  192   50   16\n",
            "    6  147 2025   19   14   22    4 1920 4613  469    4   22   71   87\n",
            "   12   16   43  530   38   76   15   13 1247    4   22   17  515   17\n",
            "   12   16  626   18    2    5   62  386   12    8  316    8  106    5\n",
            "    4 2223    2   16  480   66 3785   33    4  130   12   16   38  619\n",
            "    5   25  124   51   36  135   48   25 1415   33    6   22   12  215\n",
            "   28   77   52    5   14  407   16   82    2    8    4  107  117    2\n",
            "   15  256    4    2    7 3766    5  723   36   71   43  530  476   26\n",
            "  400  317   46    7    4    2 1029   13  104   88    4  381   15  297\n",
            "   98   32 2071   56   26  141    6  194    2   18    4  226   22   21\n",
            "  134  476   26  480    5  144   30    2   18   51   36   28  224   92\n",
            "   25  104    4  226   65   16   38 1334   88   12   16  283    5   16\n",
            " 4472  113  103   32   15   16    2   19  178   32]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beQBXgPYiw2m",
        "outputId": "3a159b49-1146-4faf-a6dc-eaaa3f2e8847"
      },
      "source": [
        "type(y_train)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgyBfPfhJxV1",
        "outputId": "dc7c42b5-1eb9-4d81-c549-c31de8f91580"
      },
      "source": [
        "# create the model\n",
        "embedding_vector_length = 32 # Embedding vector is used by the embedding layer internally\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))\n",
        "#top_words = 5000, 5000 unique words\n",
        "# input length = 500\n",
        "# input to LSTM should be of size 32, embedding_vector_length .. we can also make it 5000, or we can also keep at 100\n",
        "model.add(LSTM(100)) # 100 hidden neurons\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# loss='binary_crossentropy'--> logistic loss\n",
        "# optimizer='adam' for adaptive learning rate\n",
        "# metrics=['accuracy'], because data is balanced, for imbalanced data use Auc-ROC score\n",
        "print(model.summary())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 213,301\n",
            "Trainable params: 213,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Div3vfehh-1j",
        "outputId": "44befba7-e0d9-4cd0-9e2c-69d4bcd4689e"
      },
      "source": [
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=64)\n",
        "# every 64 reviews a backpropagation will happen because batch_size = 64"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "391/391 [==============================] - 239s 604ms/step - loss: 0.5863 - accuracy: 0.6673 - val_loss: 0.3397 - val_accuracy: 0.8533\n",
            "Epoch 2/5\n",
            "391/391 [==============================] - 236s 604ms/step - loss: 0.3936 - accuracy: 0.8255 - val_loss: 0.3704 - val_accuracy: 0.8505\n",
            "Epoch 3/5\n",
            "391/391 [==============================] - 236s 603ms/step - loss: 0.2866 - accuracy: 0.8863 - val_loss: 0.3585 - val_accuracy: 0.8485\n",
            "Epoch 4/5\n",
            "391/391 [==============================] - 237s 606ms/step - loss: 0.2535 - accuracy: 0.9009 - val_loss: 0.3112 - val_accuracy: 0.8722\n",
            "Epoch 5/5\n",
            "391/391 [==============================] - 237s 607ms/step - loss: 0.2174 - accuracy: 0.9171 - val_loss: 0.3132 - val_accuracy: 0.8723\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5a0ea5f110>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaz62GICJxV2",
        "outputId": "a2fd2a3a-2d62-4db0-d5ee-3205569cd979"
      },
      "source": [
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 87.23%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJX-Vs56k2-T"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znuSq0xlk-Bu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14208c76-042a-4eee-ba75-38ffcf989815"
      },
      "source": [
        "print('AUC on train =', roc_auc_score(y_true=y_train, y_score=model.predict_proba(X_train)[:, 0]))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
            "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AUC on train = 0.9818249024000001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAYBW4gplRll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d63d7afc-99c7-48cb-8317-ed7021a51cbd"
      },
      "source": [
        "print('AUC on test =', roc_auc_score(y_true=y_test, y_score=model.predict_proba(X_test)[:, 0]))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
            "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AUC on test = 0.9426695871999999\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}